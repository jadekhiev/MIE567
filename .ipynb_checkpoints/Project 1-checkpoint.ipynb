{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "* Put imports here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic Gridworld\n",
    "* [8 points] Now, you are asked to implement the behavior of the Gridworld environment in a Python file. Create a python file called Gridworld.py and fill in the missing details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padmanie\n",
    "class Gridworld:\n",
    "    \n",
    "    def initial_state(self):\n",
    "    # return the initial state of this MDP\n",
    "    \n",
    "    def states(self):\n",
    "    # return a list containing all the possible states in this MDP\n",
    "    \n",
    "    def reward(self, state , action , next_state ):\n",
    "    # return R(s, a, s ')\n",
    "\n",
    "    def pr(self, state , action , next_state ):\n",
    "    # return P(s' | s, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "* [15 points] Part B - Solving using Value Iteration: Given the model you obtained in Part I above, the goal now is to solve the corresponding MDP using dynamic programming. The following template has been provided for you in order to make the implementation of value iteration a little easier. You are tasked with programming the initialization and iteration of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nikita\n",
    "class ValueIteration:\n",
    "\n",
    "    def __init__(self, gridworld , gamma ):\n",
    "        # initialize the domain and discount factor\n",
    "        self. gridworld = gridworld\n",
    "        self. gamma = gamma\n",
    "\n",
    "    def initialize_values(self):\n",
    "        # you need to initialize the value function V_0[s] = 0 for all states\n",
    "        # implement V as a dictionary that maps states to values - this will guarantee constant - time reading and writing\n",
    "\n",
    "    def compute_backup(self, state , action ):\n",
    "        # use this as a helper function to return the necessary quantity E{R(s, a, S ') + gamma * V_k[S ']} in value iteration\n",
    "\n",
    "    def greedy_action(self, state ):\n",
    "        # use the previous function compute_backup to return the ( deterministic ) greedy policy pi[a | s] derived from V\n",
    "\n",
    "    def update_values(self):\n",
    "        # this method updates the value function for one iteration., e.g. V_{k}[s] -> V_{k +1}[ s] over all s\n",
    "        # first , build V_{k+1} as a dictionary\n",
    "        V_new = {}\n",
    "        # implement the rest of the procedure for computing V_new = V_{k+1}\n",
    "        # at this stage , you should use the function compute_backup and you should only modify V_new - \n",
    "        # you should read but not modify the values of self.V\n",
    "        # once V_new is computed , we can update k = k + 1 - do not modify these lines in your code\n",
    "        self.V_old , self.V = self.V, V_new\n",
    "    \n",
    "    def value_iteration (self, tolerance =1e -6) :\n",
    "        # use this to implement the complete value iteration algorithm - \n",
    "        # you should make use of the procedures initialize_values and update_values that you have written above\n",
    "        # stop your iterations when the difference between V_{k+1}(self.V) and V_k (self. V_old ) is less than tolerance\n",
    "        # this method does not need to return anything since the final V_k should be in self.V\n",
    "        # store , and when converged plot , the backed -up value function at the initial state of the MDP \n",
    "            # according to the greedy policy derived from V_k as a function of k\n",
    "        # also , print the final greedy policy in the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Value Iteration\n",
    "Provide your complete implementation of the value iteration algorithm based on this template in your report. You should prepare another Python file in which you initialize all the necessary objects (e.g. domain, algorithm) and run this file. Include all the relevant plots in your report. You should run your algorithm with the default tolerance of 1e-6 but you should also run your algorithm for different discount factor gamma IN [0.9, 0.99] and compare the plots. Provide a complete analysis of your results: \n",
    "* Did your algorithm converge at all? \n",
    "* How many iterations did this take for each value of gamma? \n",
    "* What is the best value of gamma? Also, what was the final policy you obtained for each value of gamma? \n",
    "* Is it different and why or why not? \n",
    "* Does this correspond to the optimal policy you conjectured in Part I?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run value iteration code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "* [15 points] Part C - Solving using Policy Iteration: Now, you are asked to implement the policy iteration algorithm to solve the Gridworld domain. As in Part II, you are once again provided with a template to help you structure your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jade\n",
    "class PolicyIteration :\n",
    "    \n",
    "    def __init__(self, gridworld , gamma ):\n",
    "        # initialize the domain and discount factor\n",
    "        self. gridworld = gridworld\n",
    "        self. gamma = gamma\n",
    "    \n",
    "    def initialize_policy(self):\n",
    "        # policy iteration also requires to maintain a policy\n",
    "        # initialize the policy pi_0 [s] = 0 ( LEFT ) for all states and the value function V_0[s] = 0\n",
    "        # implement the policy (self.pi) as a dictionary that maps states to actions in {LEFT , UP , RIGHT , DOWN }\n",
    "\n",
    "    def initialize_values(self):\n",
    "        # you can use the corresponding procedure you defined in ValueIteration\n",
    "\n",
    "    def compute_backup(self, state , action ):\n",
    "        # you can use the corresponding procedure you defined in ValueIteration\n",
    "    \n",
    "    def greedy_action(self, state ):\n",
    "        # use the previous function compute_backup to return the (\n",
    "        # deterministic ) greedy action a* = pi[s] derived from V\n",
    "    \n",
    "    def policy_evaluation_step(self, state ):\n",
    "        # use this function to implement the policy evaluation step for one iteration \n",
    "        #, e.g. return the value V_{k +1}[ s] = E[R(s, pi[s], S ') + gamma * V_k[S ']]\n",
    "        # you should not modify self.V\n",
    "    \n",
    "    def policy_evaluation(self, tolerance =1e -6) :\n",
    "        # in policy evaluation , we solve V = P * (R + gamma * V) for a fixed policy by iterating \n",
    "            # V_{k +1}[ s] = E[R(s, pi[s], S ') + gamma * V_k[S ']] for each state s until V_k converges uniformly\n",
    "        # use this method to perform the necessary iterations until \n",
    "            # V_k[s] (self.V) converges to within the specified tolerance\n",
    "        # this procedure should not return a value , but should modify self.V\n",
    "    def policy_improvement(self):\n",
    "        # implement the procedure here for policy improvement , \n",
    "        # e.g. pi_{k +1}[ s] = argmax_a E[R(s, a, S ') + gamma * V_k[S']]\n",
    "    \n",
    "    def policy_iteration(self, tolerance =1e -6):\n",
    "        # implement the complete policy iteration algorithm using the procedures you defined earlier\n",
    "        # your method should include: \n",
    "            # initialization of the policy and value function \n",
    "            # perform each iteration of policy iteration \n",
    "                # which includes policy evaluation followed by improvement, until the policy no longer improves\n",
    "        # you should perform monitoring and plotting the value function at the initial state , \n",
    "            # like you did for value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Policy Iteration\n",
    "Please provide a complete algorithm in your report. Please provide a similar analysis for policy iteration as you have already done for value iteration. In total, you need to run each algorithm four times to cover all the choices of gamma and p.\n",
    "Your report should follow the same format as Section 1 and answer the same questions. You are allowed to use your implementations of value and policy iteration for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run policy iteration code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Windy Gridworld\n",
    "The windy Gridworld domain is very similar to the deterministic Gridworld. However, whenever the robot is instructed to take an action, there is a probability p IN [0, 1] that the robot will actually move to the cell directly below the current cell it occupies, regardless of the action it is given, and a probability 1-p that the robot will move according to the action it is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padmanie\n",
    "class WindyGridworld:\n",
    "    \n",
    "    def initial_state(self):\n",
    "    # return the initial state of this MDP\n",
    "    \n",
    "    def states(self):\n",
    "    # return a list containing all the possible states in this MDP\n",
    "    \n",
    "    def reward(self, state , action , next_state ):\n",
    "    # return R(s, a, s ')\n",
    "\n",
    "    def pr(self, state , action , next_state ):\n",
    "    # return P(s' | s, a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
