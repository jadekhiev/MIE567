{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "* Put imports here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A-\n",
    "### Modelling\n",
    "* [7 points] Explain how you would model this navigation problem as a Markov decision process. In particular, describe:\n",
    "\n",
    "- (a) [1 points] What are the state and action spaces?\n",
    "    _The state space encompsses each of the blank cells in gridworld, namely {s11, s13*, s14, s15, s21, s23, s24, s25, s31,s32, s33, s34, s35, s41, s45, s51, s52, s53, s54} with s13 as the terminal state_ \n",
    "    _The action space is {U,D,L,R} for an attempted movement up, down, left or right, respectively_ \n",
    "    \n",
    "    \n",
    "- (b) [2 points] What is the transition probability matrix P? (You may describe just the non-zero entries.)\n",
    " *do they want us to describe a 20x20 matrix here?* \n",
    " \n",
    " \n",
    "- (c) [2 points] What is a suitable reward function R that aligns with the goal? (There are many choices, so please choose one and explain your reasoning.)\n",
    "_High value for the goal, say +25 <-- encourage finding the goal_ \n",
    "_Lose point for each time step, say -1  <-- encourage minimizing steps taken to goal (fast)_ \n",
    "_(?) high initial value, say 40 for optimistic bias (?)_ \n",
    "\n",
    "\n",
    "- (d) [2 points] Write down the discounted Bellman equation V [s] for s the initial state of this MDP. Simplify your answer as much as you can. You do not need to solve it analytically. What do you think is (are) the optimal policy (policies) for this problem, based on the Figure 1 and the system behavior?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Gridworld\n",
    "* [8 points] Now, you are asked to implement the behavior of the Gridworld environment in a Python file. Create a python file called Gridworld.py and fill in the missing details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padmanie\n",
    "class Gridworld:\n",
    "    \n",
    "    # States are a pair of row, column coordinates\n",
    "    \n",
    "    def initial_state(self):\n",
    "    # return the initial state of this MDP\n",
    "        initial_state = [5,1]        # used an array that is mutable (we can update this)\n",
    "        return initial_state\n",
    "    \n",
    "    def actions(self):\n",
    "    # return possible actions\n",
    "        actions = ['U','D','L','R']\n",
    "        return actions\n",
    "    \n",
    "    def states(self):\n",
    "    # return a list containing all the possible states in this MDP\n",
    "        states = [[1,1], [1,3], [1,4], [1,5], [2,1], [2,3], [2,4], [2,5], [3,1], [3,2],\n",
    "                    [3,3], [3,4], [3,5], [4,1], [4,5], [5,1], [5,2], [5,3], [5,4], [5,5]]\n",
    "        return states\n",
    "    \n",
    "    def reward(self, state, action, next_state):\n",
    "    # return R(s, a, s ')    \n",
    "        if next_state == [1,3] and state != [1,3]:\n",
    "            reward = 25\n",
    "        else:\n",
    "            reward = -1\n",
    "        return reward\n",
    "        \n",
    "    def next_state(self, state, action):\n",
    "        stateSpace = {(1,1):{'U':(1, 1), 'D':(2, 1), 'L':(1, 1), 'R':(1, 1)}, \n",
    "                  (1,3):{'U':(1, 3), 'D':(1, 3), 'L':(1, 3), 'R':(1, 3)}, \n",
    "                  (1,4):{'U':(1, 4), 'D':(2, 4), 'L':(1, 3), 'R':(1, 5)}, \n",
    "                  (1,5):{'U':(1, 5), 'D':(2, 5), 'L':(1, 4), 'R':(1, 5)}, \n",
    "                  (2,1):{'U':(1, 1), 'D':(3, 1), 'L':(2, 1), 'R':(2, 1)}, \n",
    "                  (2,3):{'U':(1, 3), 'D':(3, 3), 'L':(2, 3), 'R':(2, 4)}, \n",
    "                  (2,4):{'U':(1, 4), 'D':(3, 4), 'L':(2, 3), 'R':(2, 5)}, \n",
    "                  (2,5):{'U':(1, 5), 'D':(3, 5), 'L':(2, 4), 'R':(2, 5)}, \n",
    "                  (3,1):{'U':(2, 1), 'D':(4, 1), 'L':(3, 1), 'R':(3, 2)}, \n",
    "                  (3,2):{'U':(3, 2), 'D':(3, 2), 'L':(3, 1), 'R':(3, 3)},\n",
    "                  (3,3):{'U':(2, 3), 'D':(3, 3), 'L':(3, 2), 'R':(3, 4)}, \n",
    "                  (3,4):{'U':(2, 4), 'D':(3, 4), 'L':(3, 3), 'R':(3, 5)}, \n",
    "                  (3,5):{'U':(2, 5), 'D':(4, 5), 'L':(3, 4), 'R':(3, 5)}, \n",
    "                  (4,1):{'U':(3, 1), 'D':(5, 1), 'L':(4, 1), 'R':(4, 1)}, \n",
    "                  (4,5):{'U':(3, 5), 'D':(5, 5), 'L':(4, 5), 'R':(4, 5)}, \n",
    "                  (5,1):{'U':(4, 1), 'D':(5, 1), 'L':(5, 1), 'R':(5, 2)}, \n",
    "                  (5,2):{'U':(5, 2), 'D':(5, 2), 'L':(5, 1), 'R':(5, 3)}, \n",
    "                  (5,3):{'U':(5, 3), 'D':(5, 3), 'L':(5, 2), 'R':(5, 4)}, \n",
    "                  (5,4):{'U':(5, 4), 'D':(5, 4), 'L':(5, 3), 'R':(5, 5)},\n",
    "                  (5,5):{'U':(4, 5), 'D':(5, 5), 'L':(5, 4), 'R':(5, 5)}}\n",
    "        \n",
    "        return stateSpace[state][action]\n",
    "        \n",
    "    def pr(self, state, action, next_state):\n",
    "        # return P(s' | s, a)\n",
    "\n",
    "        #probabilities is a dictionary, referencing each existing state\n",
    "        # assume if in state (1,3), it will not escape\n",
    "        probabilities={(1,1):{'U':(1, 1), 'D':(2, 1), 'L':(1, 1), 'R':(1, 1)}, \n",
    "                      (1,3):{'U':(1, 3), 'D':(1, 3), 'L':(1, 3), 'R':(1, 3)}, \n",
    "                      (1,4):{'U':(1, 4), 'D':(2, 4), 'L':(1, 3), 'R':(1, 5)}, \n",
    "                      (1,5):{'U':(1, 5), 'D':(2, 5), 'L':(1, 4), 'R':(1, 5)}, \n",
    "                      (2,1):{'U':(1, 1), 'D':(3, 1), 'L':(2, 1), 'R':(2, 1)}, \n",
    "                      (2,3):{'U':(1, 3), 'D':(3, 3), 'L':(2, 3), 'R':(2, 4)}, \n",
    "                      (2,4):{'U':(1, 4), 'D':(3, 4), 'L':(2, 3), 'R':(2, 5)}, \n",
    "                      (2,5):{'U':(1, 5), 'D':(3, 5), 'L':(2, 4), 'R':(2, 5)}, \n",
    "                      (3,1):{'U':(2, 1), 'D':(4, 1), 'L':(3, 1), 'R':(3, 2)}, \n",
    "                      (3,2):{'U':(3, 2), 'D':(3, 2), 'L':(3, 1), 'R':(3, 3)},\n",
    "                      (3,3):{'U':(2, 3), 'D':(3, 3), 'L':(3, 2), 'R':(3, 4)}, \n",
    "                      (3,4):{'U':(2, 4), 'D':(3, 4), 'L':(3, 3), 'R':(3, 5)}, \n",
    "                      (3,5):{'U':(2, 5), 'D':(4, 5), 'L':(3, 4), 'R':(3, 5)}, \n",
    "                      (4,1):{'U':(3, 1), 'D':(5, 1), 'L':(4, 1), 'R':(4, 1)}, \n",
    "                      (4,5):{'U':(3, 5), 'D':(5, 5), 'L':(4, 5), 'R':(4, 5)}, \n",
    "                      (5,1):{'U':(4, 1), 'D':(5, 1), 'L':(5, 1), 'R':(5, 2)}, \n",
    "                      (5,2):{'U':(5, 2), 'D':(5, 2), 'L':(5, 1), 'R':(5, 3)}, \n",
    "                      (5,3):{'U':(5, 3), 'D':(5, 3), 'L':(5, 2), 'R':(5, 4)}, \n",
    "                      (5,4):{'U':(5, 4), 'D':(5, 4), 'L':(5, 3), 'R':(5, 5)},\n",
    "                      (5,5):{'U':(4, 5), 'D':(5, 5), 'L':(5, 4), 'R':(5, 5)}}\n",
    "        \n",
    "        if next_state == probabilities[state][action]:\n",
    "            prob = 1\n",
    "        else:\n",
    "            prob = 0\n",
    "    \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities={(1,1):{'U':(1, 1), 'D':(2, 1), 'L':(1, 1), 'R':(1, 1)}, \n",
    "              (1,3):{'U':(1, 3), 'D':(1, 3), 'L':(1, 3), 'R':(1, 3)}, \n",
    "              (1,4):{'U':(1, 4), 'D':(2, 4), 'L':(1, 3), 'R':(1, 5)}, \n",
    "              (1,5):{'U':(1, 5), 'D':(2, 5), 'L':(1, 4), 'R':(1, 5)}, \n",
    "              (2,1):{'U':(1, 1), 'D':(3, 1), 'L':(2, 1), 'R':(2, 1)}, \n",
    "              (2,3):{'U':(1, 3), 'D':(3, 3), 'L':(2, 3), 'R':(2, 4)}, \n",
    "              (2,4):{'U':(1, 4), 'D':(3, 4), 'L':(2, 3), 'R':(2, 5)}, \n",
    "              (2,5):{'U':(1, 5), 'D':(3, 5), 'L':(2, 4), 'R':(2, 5)}, \n",
    "              (3,1):{'U':(2, 1), 'D':(4, 1), 'L':(3, 1), 'R':(3, 2)}, \n",
    "              (3,2):{'U':(3, 2), 'D':(3, 2), 'L':(3, 1), 'R':(3, 3)},\n",
    "              (3,3):{'U':(2, 3), 'D':(3, 3), 'L':(3, 2), 'R':(3, 4)}, \n",
    "              (3,4):{'U':(2, 4), 'D':(3, 4), 'L':(3, 3), 'R':(3, 5)}, \n",
    "              (3,5):{'U':(2, 5), 'D':(4, 5), 'L':(3, 4), 'R':(3, 5)}, \n",
    "              (4,1):{'U':(3, 1), 'D':(5, 1), 'L':(4, 1), 'R':(4, 1)}, \n",
    "              (4,5):{'U':(3, 5), 'D':(5, 5), 'L':(4, 5), 'R':(4, 5)}, \n",
    "              (5,1):{'U':(4, 1), 'D':(5, 1), 'L':(5, 1), 'R':(5, 2)}, \n",
    "              (5,2):{'U':(5, 2), 'D':(5, 2), 'L':(5, 1), 'R':(5, 3)}, \n",
    "              (5,3):{'U':(5, 3), 'D':(5, 3), 'L':(5, 2), 'R':(5, 4)}, \n",
    "              (5,4):{'U':(5, 4), 'D':(5, 4), 'L':(5, 3), 'R':(5, 5)},\n",
    "              (5,5):{'U':(4, 5), 'D':(5, 5), 'L':(5, 4), 'R':(5, 5)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2, 1) in probabilities[(1,1)]['D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities[(1,1)]['D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "* [15 points] Part B - Solving using Value Iteration: Given the model you obtained in Part I above, the goal now is to solve the corresponding MDP using dynamic programming. The following template has been provided for you in order to make the implementation of value iteration a little easier. You are tasked with programming the initialization and iteration of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nikita\n",
    "class ValueIteration:\n",
    "\n",
    "    def __init__(self, gridworld, gamma):\n",
    "        # initialize the domain and discount factor\n",
    "        self.gridworld = gridworld\n",
    "        self.gamma = gamma\n",
    "        self.k = 0 # iteration numbers\n",
    "        self.V = {}\n",
    "\n",
    "    def initialize_values(self):\n",
    "        # you need to initialize the value function V_0[s] = 0 for all states\n",
    "        # implement V as a dictionary that maps states to values - this will guarantee constant - time reading and writing\n",
    "        V_0 = {}\n",
    "        for state in self.gridworld.states:\n",
    "            V_0[state] = 0\n",
    "        \n",
    "    def compute_backup(self, state, action):\n",
    "        # use this as a helper function to return the necessary quantity \n",
    "        # E{R(s, a, S ') + gamma * V_k[S ']} in value iteration \n",
    "        next_state = self.gridworld.next_state(state, action)  \n",
    "        return self.gridworld.reward(state, action, next_state) + self.gamma*self.gridworld.pr(state,action,next_state)*V_{k}[next_state]     \n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        # use the previous function compute_backup to return the ( deterministic ) greedy policy pi[a | s] derived from V\n",
    "        # V return the most optimal next_state for each state\n",
    "        V_temp = {}\n",
    "        for action in self.gridworld.actions:\n",
    "            V_temp[action]=compute_backup(state, action)\n",
    "        self.V[state] = max(V_temp, key=V_temp.get)\n",
    "\n",
    "    def update_values(self):\n",
    "        # this method updates the value function for one iteration., e.g. V_{k}[s] -> V_{k +1}[ s] over all s\n",
    "        # first , build V_{k+1} as a dictionary\n",
    "        # implement the rest of the procedure for computing V_new = V_{k+1}\n",
    "        V_new = {}\n",
    "        # at this stage , you should use the function compute_backup and you should only modify V_new - \n",
    "        for state in self.gridworld.states:\n",
    "            V_temp = []\n",
    "            for action in self.gridworld.actions:\n",
    "                V_temp.append(compute_backup(state, action))\n",
    "            V_new[state].update((1/4)*sum(V_temp))\n",
    "        # you should read but not modify the values of self.V\n",
    "        # once V_new is computed , we can update k = k + 1 - do not modify these lines in your code\n",
    "        # self.V_old , self.V = self.V, V_new\n",
    "        k = k + 1\n",
    "        V_{k} = V_new        \n",
    "    \n",
    "    def value_iteration (self, tolerance = 1e-6) :\n",
    "        # use this to implement the complete value iteration algorithm - \n",
    "        # you should make use of the procedures initialize_values and update_values that you have written above\n",
    "        # this method does not need to return anything since the final V_k should be in self.V\n",
    "        # store , and when converged plot , the backed -up value function at the initial state of the MDP \n",
    "            # according to the greedy policy derived from V_k as a function of k\n",
    "        # also , print the final greedy policy in the console\n",
    "        # stop your iterations when the difference between V_{k+1}(self.V) and V_k (self. V_old ) is less than tolerance\n",
    "        initalize_values() # init v(s) = 0 for all states\n",
    "        \n",
    "        while true:\n",
    "            delta = 0\n",
    "            for state in self.gridworld.states:\n",
    "                self.V_old = V_{k}\n",
    "                update_values()\n",
    "                delta = max(delta, abs(self.V - V_old))\n",
    "            if delta < tolerence: break\n",
    "\n",
    "        for state in self.gridworld.states: \n",
    "            greedy_action(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Value Iteration\n",
    "Provide your complete implementation of the value iteration algorithm based on this template in your report. You should prepare another Python file in which you initialize all the necessary objects (e.g. domain, algorithm) and run this file. Include all the relevant plots in your report. You should run your algorithm with the default tolerance of 1e-6 but you should also run your algorithm for different discount factor gamma IN [0.9, 0.99] and compare the plots. Provide a complete analysis of your results: \n",
    "* Did your algorithm converge at all? \n",
    "* How many iterations did this take for each value of gamma? \n",
    "* What is the best value of gamma? Also, what was the final policy you obtained for each value of gamma? \n",
    "* Is it different and why or why not? \n",
    "* Does this correspond to the optimal policy you conjectured in Part I?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run value iteration code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "* [15 points] Part C - Solving using Policy Iteration: Now, you are asked to implement the policy iteration algorithm to solve the Gridworld domain. As in Part II, you are once again provided with a template to help you structure your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jade\n",
    "class PolicyIteration :\n",
    "    \n",
    "    def __init__(self, gridworld , gamma ):\n",
    "        # initialize the domain and discount factor\n",
    "        self. gridworld = gridworld\n",
    "        self. gamma = gamma\n",
    "    \n",
    "    def initialize_policy(self):\n",
    "        # policy iteration also requires to maintain a policy\n",
    "        # initialize the policy pi_0 [s] = 0 ( LEFT ) for all states and the value function V_0[s] = 0\n",
    "        # implement the policy (self.pi) as a dictionary that maps states to actions in {LEFT , UP , RIGHT , DOWN }\n",
    "\n",
    "    def initialize_values(self):\n",
    "        # you can use the corresponding procedure you defined in ValueIteration\n",
    "\n",
    "    def compute_backup(self, state , action ):\n",
    "        # you can use the corresponding procedure you defined in ValueIteration\n",
    "    \n",
    "    def greedy_action(self, state ):\n",
    "        # use the previous function compute_backup to return the (\n",
    "        # deterministic ) greedy action a* = pi[s] derived from V\n",
    "    \n",
    "    def policy_evaluation_step(self, state ):\n",
    "        # use this function to implement the policy evaluation step for one iteration \n",
    "        #, e.g. return the value V_{k +1}[ s] = E[R(s, pi[s], S ') + gamma * V_k[S ']]\n",
    "        # you should not modify self.V\n",
    "    \n",
    "    def policy_evaluation(self, tolerance =1e -6) :\n",
    "        # in policy evaluation , we solve V = P * (R + gamma * V) for a fixed policy by iterating \n",
    "            # V_{k +1}[ s] = E[R(s, pi[s], S ') + gamma * V_k[S ']] for each state s until V_k converges uniformly\n",
    "        # use this method to perform the necessary iterations until \n",
    "            # V_k[s] (self.V) converges to within the specified tolerance\n",
    "        # this procedure should not return a value , but should modify self.V\n",
    "    def policy_improvement(self):\n",
    "        # implement the procedure here for policy improvement , \n",
    "        # e.g. pi_{k +1}[ s] = argmax_a E[R(s, a, S ') + gamma * V_k[S']]\n",
    "    \n",
    "    def policy_iteration(self, tolerance =1e -6):\n",
    "        # implement the complete policy iteration algorithm using the procedures you defined earlier\n",
    "        # your method should include: \n",
    "            # initialization of the policy and value function \n",
    "            # perform each iteration of policy iteration \n",
    "                # which includes policy evaluation followed by improvement, until the policy no longer improves\n",
    "        # you should perform monitoring and plotting the value function at the initial state , \n",
    "            # like you did for value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Policy Iteration\n",
    "Please provide a complete algorithm in your report. Please provide a similar analysis for policy iteration as you have already done for value iteration. In total, you need to run each algorithm four times to cover all the choices of gamma and p.\n",
    "Your report should follow the same format as Section 1 and answer the same questions. You are allowed to use your implementations of value and policy iteration for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run policy iteration code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Windy Gridworld\n",
    "The windy Gridworld domain is very similar to the deterministic Gridworld. However, whenever the robot is instructed to take an action, there is a probability p IN [0, 1] that the robot will actually move to the cell directly below the current cell it occupies, regardless of the action it is given, and a probability 1-p that the robot will move according to the action it is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padmanie\n",
    "class WindyGridworld:\n",
    "    \n",
    "    # States are a pair of row, column coordinates\n",
    "    \n",
    "    def initial_state(self):\n",
    "    # return the initial state of this MDP\n",
    "        initial_state = [5,1]        # used an array that is mutable (we can update this)\n",
    "    \n",
    "    def states(self):\n",
    "    # return a list containing all the possible states in this MDP\n",
    "        self.states = [[1,1], [1,3], [1,4], [1,5], [2,1], [2,3], [2,4], [2,5], [3,1], [3,2],\n",
    "                       [3,3], [3,4], [3,5], [4,1], [4,5], [5,1], [5,2], [5,3], [5,4], [5,5]]\n",
    "    \n",
    "    def reward(self, state , action , next_state ):\n",
    "    # return R(s, a, s ')\n",
    "        if next_state == [1,3] and state != [1,3]:\n",
    "            reward = 25\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "    def pr(self, state , action , next_state ):\n",
    "    # return P(s' | s, a)\n",
    "            #probabilities is a dictionary, referencing each existing state\n",
    "        # assume if in state (1,3), it will not escape\n",
    "        probabilities{(1,1):{'U':(1, 1), 'D':(2, 1), 'L':(1, 1), 'R':(1, 1)}, \n",
    "                      (1,3):{'U':(1, 3), 'D':(1, 3), 'L':(1, 3), 'R':(1, 3)}, \n",
    "                      (1,4):{'U':(1, 4), 'D':(2, 4), 'L':(1, 3), 'R':(1, 5)}, \n",
    "                      (1,5):{'U':(1, 5), 'D':(2, 5), 'L':(1, 4), 'R':(1, 5)}, \n",
    "                      (2,1):{'U':(1, 1), 'D':(3, 1), 'L':(2, 1), 'R':(2, 1)}, \n",
    "                      (2,3):{'U':(1, 3), 'D':(3, 3), 'L':(2, 3), 'R':(2, 4)}, \n",
    "                      (2,4):{'U':(1, 4), 'D':(3, 4), 'L':(2, 3), 'R':(2, 5)}, \n",
    "                      (2,5):{'U':(1, 5), 'D':(3, 5), 'L':(2, 4), 'R':(2, 5)}, \n",
    "                      (3,1):{'U':(2, 1), 'D':(4, 1), 'L':(3, 1), 'R':(3, 2)}, \n",
    "                      (3,2):{'U':(3, 2), 'D':(3, 2), 'L':(3, 1), 'R':(3, 3)},\n",
    "                      (3,3):{'U':(2, 3), 'D':(3, 3), 'L':(3, 2), 'R':(3, 4)}, \n",
    "                      (3,4):{'U':(2, 4), 'D':(3, 4), 'L':(3, 3), 'R':(3, 5)}, \n",
    "                      (3,5):{'U':(2, 5), 'D':(4, 5), 'L':(3, 4), 'R':(3, 5)}, \n",
    "                      (4,1):{'U':(3, 1), 'D':(5, 1), 'L':(4, 1), 'R':(4, 1)}, \n",
    "                      (4,5):{'U':(3, 5), 'D':(5, 5), 'L':(4, 5), 'R':(4, 5)}, \n",
    "                      (5,1):{'U':(4, 1), 'D':(5, 1), 'L':(5, 1), 'R':(5, 2)}, \n",
    "                      (5,2):{'U':(5, 2), 'D':(5, 2), 'L':(5, 1), 'R':(5, 3)}, \n",
    "                      (5,3):{'U':(5, 3), 'D':(5, 3), 'L':(5, 2), 'R':(5, 4)}, \n",
    "                      (5,4):{'U':(5, 4), 'D':(5, 4), 'L':(5, 3), 'R':(5, 5)}\n",
    "                      (5,5):{'U':(4, 5), 'D':(5, 5), 'L':(5, 4), 'R':(5, 5)}}\n",
    "        if next_state in probabilities[state][action]:\n",
    "            prob = 1-p\n",
    "        else:\n",
    "            prob = 0\n",
    "    \n",
    "    return prob\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
