{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flags Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Flags domain, time is broken up into discrete decision periods, and the world is a square area that is broken up into a 5-by-5 grid of cells, as illustrated in Figure 1 below. \n",
    "<img src=\"flagworld.png\">\n",
    "At the beginning of each period, a robot occupies one of the available cells. It is then allowed to choose a direction of movement (up, down, left or right), and moves to the corresponding adjacent cell in that direction. However, if the chosen action would take the robot outside the boundaries of the grid, the robot does not move but remains in the current cell. So far, the problem is just like the Gridworld domain you have already encountered in your first assignment. In each run, the robot begins in state S. <br>\n",
    "As illustrated in Figure 1, instead of a single destination G, there are five different flags, numbered 1, 2, 3, 4 and G, that must be collected in the correct order (first 1, then 2, etc... to G). Once the robot has collected all the  flags in the correct order and lands on cell G, the task is complete. If the robot collects the wrong flag at the wrong time (e.g. tries to collect 2 first, then 1...), the robot does not have to start over. Instead, the robot is penalized for taking this action, and the wrong flag is not collected (you may assume for training purposes, that the flags that have already been collected so far are kept). The main objective is to collect all the flags in the correct order in the least possible amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flags:\n",
    "    def initial_state(self):\n",
    "    # return the initial state of this MDP\n",
    "        initial_state = (0,5,1)\n",
    "        return initial_state\n",
    "\n",
    "    def get_all_states(self):\n",
    "    # return a list containing all the possible states in this MDP\n",
    "        states = [\n",
    "                (0,1,1),    (1,1,1),    (2,1,1),    (3,1,1),    (4,1,1),\n",
    "                (0,1,2),    (1,1,2),    (2,1,2),    (3,1,2),    (4,1,2),\n",
    "                (0,1,3),    (1,1,3),    (2,1,3),    (3,1,3),    (4,1,3),\n",
    "                (0,1,4),    (1,1,4),    (2,1,4),    (3,1,4),    (4,1,4),\n",
    "                (0,1,5),    (1,1,5),    (2,1,5),    (3,1,5),\n",
    "                (0,2,1),    (1,2,1),    (2,2,1),    (3,2,1),    (4,2,1),\n",
    "                (0,2,2),                (2,2,2),    (3,2,2),    (4,2,2),\n",
    "                (0,2,3),    (1,2,3),    (2,2,3),    (3,2,3),    (4,2,3),\n",
    "                (0,2,4),    (1,2,4),    (2,2,4),    (3,2,4),    (4,2,4),\n",
    "                (0,2,5),    (1,2,5),    (2,2,5),    (3,2,5),    (4,2,5),\n",
    "                (0,3,1),    (1,3,1),    (2,3,1),    (3,3,1),    (4,3,1),\n",
    "                (0,3,2),    (1,3,2),    (2,3,2),    (3,3,2),    (4,3,2),\n",
    "                (0,3,3),    (1,3,3),    (2,3,3),    (3,3,3),    (4,3,3),\n",
    "                (0,3,4),    (1,3,4),    (2,3,4),    (3,3,4),    (4,3,4),\n",
    "                (0,3,5),    (1,3,5),    (2,3,5),    (3,3,5),    (4,3,5),\n",
    "                (0,4,1),    (1,4,1),                (3,4,1),    (4,4,1),\n",
    "                (0,4,2),    (1,4,2),    (2,4,2),    (3,4,2),    (4,4,2),\n",
    "                (0,4,3),    (1,4,3),    (2,4,3),    (3,4,3),    (4,4,3),\n",
    "                (0,4,4),    (1,4,4),    (2,4,4),    (3,4,4),    (4,4,4),\n",
    "                            (1,4,5),    (2,4,5),    (3,4,5),    (4,4,5),\n",
    "                (0,5,1),    (1,5,1),    (2,5,1),    (3,5,1),    (4,5,1),\n",
    "                (0,5,2),    (1,5,2),    (2,5,2),    (3,5,2),    (4,5,2),\n",
    "                (0,5,3),    (1,5,3),    (2,5,3),    (3,5,3),    (4,5,3),\n",
    "                (0,5,4),    (1,5,4),    (2,5,4),    (3,5,4),    (4,5,4),\n",
    "                (0,5,5),    (1,5,5),    (2,5,5),                (4,5,5),\n",
    "\n",
    "                (5,1,5) # TERMINAL STATE\n",
    "                 ]\n",
    "        return states\n",
    "    \n",
    "    def actions(self):\n",
    "    # return possible actions\n",
    "        actions = ['U','D','L','R']\n",
    "        return actions\n",
    "    \n",
    "    def next_state(self, state, action):\n",
    "        stateSpace ={ # ( number of flags captured, row index, column index )\n",
    "                    (0,1,1):{'U':(0,1,1),'D':(0,2,1),'L':(0,1,1),'R':(0,1,2)},\n",
    "                    (0,1,2):{'U':(0,1,2),'D':(0,2,2),'L':(0,1,1),'R':(0,1,3)},\n",
    "                    (0,1,3):{'U':(0,1,3),'D':(0,2,3),'L':(0,1,2),'R':(0,1,4)},\n",
    "                    (0,1,4):{'U':(0,1,4),'D':(0,2,4),'L':(0,1,3),'R':(0,1,5)},\n",
    "                    (0,1,5):{'U':(0,1,5),'D':(0,2,5),'L':(0,1,4),'R':(0,1,5)},\n",
    "                    (0,2,1):{'U':(0,1,1),'D':(0,3,1),'L':(0,2,1),'R':(0,2,2)},\n",
    "                    (0,2,2):{'U':(0,1,2),'D':(0,3,2),'L':(0,2,1),'R':(0,2,3)},\n",
    "                    (0,2,3):{'U':(0,1,3),'D':(0,3,3),'L':(0,2,2),'R':(0,2,4)},\n",
    "                    (0,2,4):{'U':(0,1,4),'D':(0,3,4),'L':(0,2,3),'R':(0,2,5)},\n",
    "                    (0,2,5):{'U':(0,1,5),'D':(0,3,5),'L':(0,2,4),'R':(0,2,5)},\n",
    "                    (0,3,1):{'U':(0,2,1),'D':(0,4,1),'L':(0,3,1),'R':(0,3,2)},\n",
    "                    (0,3,2):{'U':(0,2,2),'D':(0,4,2),'L':(0,3,1),'R':(0,3,3)},\n",
    "                    (0,3,3):{'U':(0,2,3),'D':(0,4,3),'L':(0,3,2),'R':(0,3,4)},\n",
    "                    (0,3,4):{'U':(0,2,4),'D':(0,4,4),'L':(0,3,3),'R':(0,3,5)},\n",
    "                    (0,3,5):{'U':(0,2,5),'D':(1,4,5),'L':(0,3,4),'R':(0,3,5)}, # flag captured {'D'}\n",
    "                    (0,4,1):{'U':(0,3,1),'D':(0,5,1),'L':(0,4,1),'R':(0,4,2)},\n",
    "                    (0,4,2):{'U':(0,3,2),'D':(0,5,2),'L':(0,4,1),'R':(0,4,3)},\n",
    "                    (0,4,3):{'U':(0,3,3),'D':(0,5,3),'L':(0,4,2),'R':(0,4,4)},\n",
    "                    (0,4,4):{'U':(0,3,4),'D':(0,5,4),'L':(0,4,3),'R':(1,4,5)}, # flag captured {'R'}\n",
    "                    (0,5,1):{'U':(0,4,1),'D':(0,5,1),'L':(0,5,1),'R':(0,5,2)},\n",
    "                    (0,5,2):{'U':(0,4,2),'D':(0,5,2),'L':(0,5,1),'R':(0,5,3)},\n",
    "                    (0,5,3):{'U':(0,4,3),'D':(0,5,3),'L':(0,5,2),'R':(0,5,4)},\n",
    "                    (0,5,4):{'U':(0,4,4),'D':(0,5,4),'L':(0,5,3),'R':(0,5,5)},\n",
    "                    (0,5,5):{'U':(1,4,5),'D':(0,5,5),'L':(0,5,4),'R':(0,5,5)}, # flag captured {'U'}\n",
    "                    \n",
    "                    (1,1,1):{'U':(1,1,1),'D':(1,2,1),'L':(1,1,1),'R':(1,1,2)},\n",
    "                    (1,1,2):{'U':(1,1,2),'D':(2,2,2),'L':(1,1,1),'R':(1,1,3)}, # flag captured {'D'}\n",
    "                    (1,1,3):{'U':(1,1,3),'D':(1,2,3),'L':(1,1,2),'R':(1,1,4)},\n",
    "                    (1,1,4):{'U':(1,1,4),'D':(1,2,4),'L':(1,1,3),'R':(1,1,5)},\n",
    "                    (1,1,5):{'U':(1,1,5),'D':(1,2,5),'L':(1,1,4),'R':(1,1,5)},\n",
    "                    (1,2,1):{'U':(1,1,1),'D':(1,3,1),'L':(1,2,1),'R':(2,2,2)}, # flag captured {'R'}\n",
    "                    (1,2,3):{'U':(1,1,3),'D':(1,3,3),'L':(2,2,2),'R':(1,2,4)}, # flag captured {'L'}\n",
    "                    (1,2,4):{'U':(1,1,4),'D':(1,3,4),'L':(1,2,3),'R':(1,2,5)},\n",
    "                    (1,2,5):{'U':(1,1,5),'D':(1,3,5),'L':(1,2,4),'R':(1,2,5)},\n",
    "                    (1,3,1):{'U':(1,2,1),'D':(1,4,1),'L':(1,3,1),'R':(1,3,2)},\n",
    "                    (1,3,2):{'U':(2,2,2),'D':(1,4,2),'L':(1,3,1),'R':(1,3,3)}, # flag captured {'U'}\n",
    "                    (1,3,3):{'U':(1,2,3),'D':(1,4,3),'L':(1,3,2),'R':(1,3,4)},\n",
    "                    (1,3,4):{'U':(1,2,4),'D':(1,4,4),'L':(1,3,3),'R':(1,3,5)},\n",
    "                    (1,3,5):{'U':(1,2,5),'D':(1,4,5),'L':(1,3,4),'R':(1,3,5)},\n",
    "                    (1,4,1):{'U':(1,3,1),'D':(1,5,1),'L':(1,4,1),'R':(1,4,2)},\n",
    "                    (1,4,2):{'U':(1,3,2),'D':(1,5,2),'L':(1,4,1),'R':(1,4,3)},\n",
    "                    (1,4,3):{'U':(1,3,3),'D':(1,5,3),'L':(1,4,2),'R':(1,4,4)},\n",
    "                    (1,4,4):{'U':(1,3,4),'D':(1,5,4),'L':(1,4,3),'R':(1,4,5)},\n",
    "                    (1,4,5):{'U':(1,3,5),'D':(1,5,5),'L':(1,4,4),'R':(1,4,5)},\n",
    "                    (1,5,1):{'U':(1,4,1),'D':(1,5,1),'L':(1,5,1),'R':(1,5,2)},\n",
    "                    (1,5,2):{'U':(1,4,2),'D':(1,5,2),'L':(1,5,1),'R':(1,5,3)},\n",
    "                    (1,5,3):{'U':(1,4,3),'D':(1,5,3),'L':(1,5,2),'R':(1,5,4)},\n",
    "                    (1,5,4):{'U':(1,4,4),'D':(1,5,4),'L':(1,5,3),'R':(1,5,5)},\n",
    "                    (1,5,5):{'U':(1,4,5),'D':(1,5,5),'L':(1,5,4),'R':(1,5,5)},\n",
    "                    \n",
    "                    (2,1,1):{'U':(2,1,1),'D':(2,2,1),'L':(2,1,1),'R':(2,1,2)},\n",
    "                    (2,1,2):{'U':(2,1,2),'D':(2,2,2),'L':(2,1,1),'R':(2,1,3)},\n",
    "                    (2,1,3):{'U':(2,1,3),'D':(2,2,3),'L':(2,1,2),'R':(2,1,4)},\n",
    "                    (2,1,4):{'U':(2,1,4),'D':(2,2,4),'L':(2,1,3),'R':(2,1,5)},\n",
    "                    (2,1,5):{'U':(2,1,5),'D':(2,2,5),'L':(2,1,4),'R':(2,1,5)},\n",
    "                    (2,2,1):{'U':(2,1,1),'D':(2,3,1),'L':(2,2,1),'R':(2,2,2)},\n",
    "                    (2,2,2):{'U':(2,1,2),'D':(2,3,2),'L':(2,2,1),'R':(2,2,3)},\n",
    "                    (2,2,3):{'U':(2,1,3),'D':(2,3,3),'L':(2,2,2),'R':(2,2,4)},\n",
    "                    (2,2,4):{'U':(2,1,4),'D':(2,3,4),'L':(2,2,3),'R':(2,2,5)},\n",
    "                    (2,2,5):{'U':(2,1,5),'D':(2,3,5),'L':(2,2,4),'R':(2,2,5)},\n",
    "                    (2,3,1):{'U':(2,2,1),'D':(3,4,1),'L':(2,3,1),'R':(2,3,2)}, \n",
    "                    (2,3,2):{'U':(2,2,2),'D':(2,4,2),'L':(2,3,1),'R':(2,3,3)}, # flag captured {'D'}\n",
    "                    (2,3,3):{'U':(2,2,3),'D':(2,4,3),'L':(2,3,2),'R':(2,3,4)},\n",
    "                    (2,3,4):{'U':(2,2,4),'D':(2,4,4),'L':(2,3,3),'R':(2,3,5)},\n",
    "                    (2,3,5):{'U':(2,2,5),'D':(2,4,5),'L':(2,3,4),'R':(2,3,5)},\n",
    "                    (2,4,2):{'U':(2,3,2),'D':(2,5,2),'L':(3,4,1),'R':(2,4,3)}, # flag captured {'L'}\n",
    "                    (2,4,3):{'U':(2,3,3),'D':(2,5,3),'L':(2,4,2),'R':(2,4,4)},\n",
    "                    (2,4,4):{'U':(2,3,4),'D':(2,5,4),'L':(2,4,3),'R':(2,4,5)},\n",
    "                    (2,4,5):{'U':(2,3,5),'D':(2,5,5),'L':(2,4,4),'R':(2,4,5)},\n",
    "                    (2,5,1):{'U':(3,4,1),'D':(2,5,1),'L':(2,5,1),'R':(2,5,2)}, # flag captured {'U'}\n",
    "                    (2,5,2):{'U':(2,4,2),'D':(2,5,2),'L':(2,5,1),'R':(2,5,3)},\n",
    "                    (2,5,3):{'U':(2,4,3),'D':(2,5,3),'L':(2,5,2),'R':(2,5,4)},\n",
    "                    (2,5,4):{'U':(2,4,4),'D':(2,5,4),'L':(2,5,3),'R':(2,5,5)},\n",
    "                    (2,5,5):{'U':(2,4,5),'D':(2,5,5),'L':(2,5,4),'R':(2,5,5)},\n",
    "                    \n",
    "                    (3,1,1):{'U':(3,1,1),'D':(3,2,1),'L':(3,1,1),'R':(3,1,2)},\n",
    "                    (3,1,2):{'U':(3,1,2),'D':(3,2,2),'L':(3,1,1),'R':(3,1,3)},\n",
    "                    (3,1,3):{'U':(3,1,3),'D':(3,2,3),'L':(3,1,2),'R':(3,1,4)},\n",
    "                    (3,1,4):{'U':(3,1,4),'D':(3,2,4),'L':(3,1,3),'R':(3,1,5)},\n",
    "                    (3,1,5):{'U':(3,1,5),'D':(3,2,5),'L':(3,1,4),'R':(3,1,5)},\n",
    "                    (3,2,1):{'U':(3,1,1),'D':(3,3,1),'L':(3,2,1),'R':(3,2,2)},\n",
    "                    (3,2,2):{'U':(3,1,2),'D':(3,3,2),'L':(3,2,1),'R':(3,2,3)},\n",
    "                    (3,2,3):{'U':(3,1,3),'D':(3,3,3),'L':(3,2,2),'R':(3,2,4)},\n",
    "                    (3,2,4):{'U':(3,1,4),'D':(3,3,4),'L':(3,2,3),'R':(3,2,5)},\n",
    "                    (3,2,5):{'U':(3,1,5),'D':(3,3,5),'L':(3,2,4),'R':(3,2,5)},\n",
    "                    (3,3,1):{'U':(3,2,1),'D':(3,4,1),'L':(3,3,1),'R':(3,3,2)},\n",
    "                    (3,3,2):{'U':(3,2,2),'D':(3,4,2),'L':(3,3,1),'R':(3,3,3)},\n",
    "                    (3,3,3):{'U':(3,2,3),'D':(3,4,3),'L':(3,3,2),'R':(3,3,4)},\n",
    "                    (3,3,4):{'U':(3,2,4),'D':(3,4,4),'L':(3,3,3),'R':(3,3,5)},\n",
    "                    (3,3,5):{'U':(3,2,5),'D':(3,4,5),'L':(3,3,4),'R':(3,3,5)},\n",
    "                    (3,4,1):{'U':(3,3,1),'D':(3,5,1),'L':(3,4,1),'R':(3,4,2)},\n",
    "                    (3,4,2):{'U':(3,3,2),'D':(3,5,2),'L':(3,4,1),'R':(3,4,3)},\n",
    "                    (3,4,3):{'U':(3,3,3),'D':(3,5,3),'L':(3,4,2),'R':(3,4,4)},\n",
    "                    (3,4,4):{'U':(3,3,4),'D':(3,5,4),'L':(3,4,3),'R':(3,4,5)},\n",
    "                    (3,4,5):{'U':(3,3,5),'D':(4,5,5),'L':(3,4,4),'R':(3,4,5)}, # flag captured {'D'}\n",
    "                    (3,5,1):{'U':(3,4,1),'D':(3,5,1),'L':(3,5,1),'R':(3,5,2)},\n",
    "                    (3,5,2):{'U':(3,4,2),'D':(3,5,2),'L':(3,5,1),'R':(3,5,3)},\n",
    "                    (3,5,3):{'U':(3,4,3),'D':(3,5,3),'L':(3,5,2),'R':(3,5,4)},\n",
    "                    (3,5,4):{'U':(3,4,4),'D':(3,5,4),'L':(3,5,3),'R':(4,5,5)}, # flag captured {'R'}\n",
    "                    \n",
    "                    (4,1,1):{'U':(4,1,1),'D':(4,2,1),'L':(4,1,1),'R':(4,1,2)},\n",
    "                    (4,1,2):{'U':(4,1,2),'D':(4,2,2),'L':(4,1,1),'R':(4,1,3)},\n",
    "                    (4,1,3):{'U':(4,1,3),'D':(4,2,3),'L':(4,1,2),'R':(4,1,4)},\n",
    "                    (4,1,4):{'U':(4,1,4),'D':(4,2,4),'L':(4,1,3),'R':(5,1,5)}, # flag captured {'R'}\n",
    "                    (4,2,1):{'U':(4,1,1),'D':(4,3,1),'L':(4,2,1),'R':(4,2,2)},\n",
    "                    (4,2,2):{'U':(4,1,2),'D':(4,3,2),'L':(4,2,1),'R':(4,2,3)},\n",
    "                    (4,2,3):{'U':(4,1,3),'D':(4,3,3),'L':(4,2,2),'R':(4,2,4)},\n",
    "                    (4,2,4):{'U':(4,1,4),'D':(4,3,4),'L':(4,2,3),'R':(4,2,5)},\n",
    "                    (4,2,5):{'U':(5,1,5),'D':(4,3,5),'L':(4,2,4),'R':(4,2,5)}, # flag captured {'U'}\n",
    "                    (4,3,1):{'U':(4,2,1),'D':(4,4,1),'L':(4,3,1),'R':(4,3,2)},\n",
    "                    (4,3,2):{'U':(4,2,2),'D':(4,4,2),'L':(4,3,1),'R':(4,3,3)},\n",
    "                    (4,3,3):{'U':(4,2,3),'D':(4,4,3),'L':(4,3,2),'R':(4,3,4)},\n",
    "                    (4,3,4):{'U':(4,2,4),'D':(4,4,4),'L':(4,3,3),'R':(4,3,5)},\n",
    "                    (4,3,5):{'U':(4,2,5),'D':(4,4,5),'L':(4,3,4),'R':(4,3,5)},\n",
    "                    (4,4,1):{'U':(4,3,1),'D':(4,5,1),'L':(4,4,1),'R':(4,4,2)},\n",
    "                    (4,4,2):{'U':(4,3,2),'D':(4,5,2),'L':(4,4,1),'R':(4,4,3)},\n",
    "                    (4,4,3):{'U':(4,3,3),'D':(4,5,3),'L':(4,4,2),'R':(4,4,4)},\n",
    "                    (4,4,4):{'U':(4,3,4),'D':(4,5,4),'L':(4,4,3),'R':(4,4,5)},\n",
    "                    (4,4,5):{'U':(4,3,5),'D':(4,5,5),'L':(4,4,4),'R':(4,4,5)},\n",
    "                    (4,5,1):{'U':(4,4,1),'D':(4,5,1),'L':(4,5,1),'R':(4,5,2)},\n",
    "                    (4,5,2):{'U':(4,4,2),'D':(4,5,2),'L':(4,5,1),'R':(4,5,3)},\n",
    "                    (4,5,3):{'U':(4,4,3),'D':(4,5,3),'L':(4,5,2),'R':(4,5,4)},\n",
    "                    (4,5,4):{'U':(4,4,4),'D':(4,5,4),'L':(4,5,3),'R':(4,5,5)},\n",
    "                    (4,5,5):{'U':(4,4,5),'D':(4,5,5),'L':(4,5,4),'R':(4,5,5)}\n",
    "                    }\n",
    "        \n",
    "        return stateSpace[state][action]\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "    # this function should return a Boolean indicating\n",
    "    # whether or not state is a terminal state ; \n",
    "    # in other words , does the game end at the specified state\n",
    "    # or does the robot keep playing ?\n",
    "    # TERMINAL STATE IS (5,1,5) - returns True if algo should ends o/w returns False\n",
    "    # use following line in your algo to use this function\n",
    "    # self.domain.is_terminal(state):\n",
    "        if state == (5,1,5): return True\n",
    "        else: return False\n",
    "    \n",
    "\n",
    "    def transition(self, state , action):\n",
    "    # this function should simulate the intended behavior of the Flags domain\n",
    "    # in particular , given the specified state and action, \n",
    "    # this function should return a tuple containing two things :\n",
    "    # 1. the next state to which we are transitioning to in the next period\n",
    "    # 2. the reward obtained according to your reward function when transitioning to the next state    \n",
    "            \"\"\"\n",
    "            Flag states: \n",
    "            1. (4,5) \n",
    "            2. (2,2)\n",
    "            3. (4,1)\n",
    "            4. (5,5)\n",
    "            5. (1,5)\n",
    "            - Assume punishment for visiting any of the flag states prematurely\n",
    "            - Assume no punishment for revisiting a state that previously held a captured flag\n",
    "            \"\"\"\n",
    "        flagStates = [(4,5),(2,2),(4,1),(5,5),(1,5)]\n",
    "        nextState = self.next_state(state,action)\n",
    "        \n",
    "        # state[0] = number of flags \n",
    "        if state[0] == nextState[0]:\n",
    "            \n",
    "            # if flag state hit prematurely: -10 \n",
    "            # next spot on grid = (nextState[1], nextState[2]) = (row #, col #)\n",
    "            # flagStates[state[0]:4] <- slice the list to only include the flags not yet captured\n",
    "                # flagState = ordered list containing the all the flag states\n",
    "                # [state[0]:4] = [number of flags captured : end of list] = subset representative of remaining flags\n",
    "                # if we've captured 4 flags, we no longer have this restriction i.e. no flags left on board\n",
    "            if state[0] != 4 and (nextState[1],nextState[2]) in flagStates[state[0]:5]: reward = -10\n",
    "            \n",
    "            # if no flag captured: -1\n",
    "            else: reward = -1\n",
    "\n",
    "        # if flag captured: +25\n",
    "        else: reward = 25\n",
    "\n",
    "        return nextState, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-Visit On-Policy MC\n",
    "For this algorithm, like all the others that follow, you train using the epsilon-greedy policy. You are allowed to fix the discount factor gamma to 0.99. Set the maximum length of each episode to 200 if the terminal state is not reached by this point. Training should proceed for a total of 2000 episodes. You need to experiment with the epsilon parameter in epsilon greedy. Run your experiment for different choices of this parameter in {0.01, 0:1, 0.25} and include plots in your report for each of these values. What is the best value of epsilon? Why? For each epsilon, did your algorithm converge at all? What was the final policy and Q-values that you typically obtain (typically, as in the majority of the trials)? Does this correspond to an optimal policy?\n",
    "<img src=\"fv-on-MC.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "class MonteCarlo:\n",
    "    def __init__(self, domain, gamma, epsilon):\n",
    "    # sets the domain instance, and all relevant parameters for each algorithm \n",
    "    # (e.g. learning rate, epsilon, etc.).\n",
    "\n",
    "    def initialize_values(self):\n",
    "    # A procedure called initialize values that initializes the Q-values to zero for all\n",
    "    # states in the MDP and all relevant other counters and memory.\n",
    "    \n",
    "    def sample_epsilon_greedy(self):\n",
    "    # takes a state of the MDP as argument and returns an action sampled \n",
    "    # according to the epsilon-greedy policy defined by the epsilon parameter \n",
    "    # set in the init method and the Q-values.\n",
    "    \n",
    "    def test_greedy(self): \n",
    "    # generates an episode from the MDP using the greedy policy \n",
    "    # (we are testing, so please do NOT use the epsilon-greedy policy for this!) \n",
    "    # with respect to the current Q-values, and returns G0, the discounted return \n",
    "    # starting from the initial state (please refer to your lecture or tutorial notes). \n",
    "    # This function will be used to inspect the rate at which your algorithm \n",
    "    # converges to a good policy as it trains.\n",
    "    \n",
    "    def train_episode(self):\n",
    "    # Each algorithm has a main training loop that generates an episode from the MDP\n",
    "    # according to the epsilon-greedy policy and uses this to update the Q-values. \n",
    "    # How you implement this method is up to you, but it is easiest to refer to the pseudo-code\n",
    "    # of each algorithm discussed in class or in the Sutton and Barto book.\n",
    "    \n",
    "    def train_trial(self):\n",
    "    # Training typically does not only happen for one episode, but rather for many episodes. \n",
    "    # Create a function that first initializes your Q-values using your method initialize values,\n",
    "    # and then proceeds to train using a predetermined number of episodes using your method train episode \n",
    "    # In addition, at the end of each episode, you should test the greedy policy you've obtained \n",
    "    # using your method test_greedy. \n",
    "    # Keep track of this progress in a list or a numpy array, and\n",
    "    # return it in this procedure once training completes\n",
    "    \n",
    "    \"\"\"\n",
    "The algorithms in model-free RL are inherently random due to the randomness of\n",
    "the exploration policy and possibly the stochasticity of the MDP (not relevant here,\n",
    "since it is deterministic). In order to get a better idea of how fast each algorithm\n",
    "converges, practitioners typically run each algorithm independently using the same\n",
    "initial conditions many times (try to do it for at least 20 trials if you can: if your\n",
    "computer is too slow, you may reduce it to 10), and average the results from all the\n",
    "trials. In order to do this, you will have to run your method train trial many\n",
    "times. Record the per-episode performance data returned by that method over all\n",
    "trials. At the end, compute both the sample mean and standard deviation over trials\n",
    "for each episode. Plot these result in separate figures with episode on the x-axis and\n",
    "the mean/std of returns on the y-axis.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each algorithm you implement, you should have another Python file that allows to\n",
    "run your experiment. Here, you will have to initialize your domain, and the algorithm\n",
    "with your parameter settings, and run your algorithm.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning (Off-Policy TD)\n",
    "Implement the off-policy Q-learning algorithm as discussed in class. You may fix the learning rate to alpha = 0.3 for all your experiments. Set gamma = 0.99, the maximum episode length to 200 time steps, and the number of episodes of training to 500. Experiment with epsilon in {0.01, 0.1, 0.25}, answer the same questions and provide the same information as Part B.\n",
    "<img src=\"ql.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "class QLearning:\n",
    "    def __init__(self, domain, alpha, gamma, epsilon):\n",
    "    # sets the domain instance, and all relevant parameters for each algorithm \n",
    "    # (e.g. learning rate, epsilon, etc.).\n",
    "\n",
    "    def initialize_values(self):\n",
    "    # A procedure called initialize values that initializes the Q-values to zero for all\n",
    "    # states in the MDP and all relevant other counters and memory.\n",
    "    \n",
    "    def sample_epsilon_greedy(self):\n",
    "    # takes a state of the MDP as argument and returns an action sampled \n",
    "    # according to the epsilon-greedy policy defined by the epsilon parameter \n",
    "    # set in the init method and the Q-values.\n",
    "    \n",
    "    def test_greedy(self): \n",
    "    # generates an episode from the MDP using the greedy policy \n",
    "    # (we are testing, so please do NOT use the epsilon-greedy policy for this!) \n",
    "    # with respect to the current Q-values, and returns G0, the discounted return \n",
    "    # starting from the initial state (please refer to your lecture or tutorial notes). \n",
    "    # This function will be used to inspect the rate at which your algorithm \n",
    "    # converges to a good policy as it trains.\n",
    "    \n",
    "    def train_episode(self):\n",
    "    # Each algorithm has a main training loop that generates an episode from the MDP\n",
    "    # according to the epsilon-greedy policy and uses this to update the Q-values. \n",
    "    # How you implement this method is up to you, but it is easiest to refer to the pseudo-code\n",
    "    # of each algorithm discussed in class or in the Sutton and Barto book.\n",
    "    \n",
    "    def train_trial(self):\n",
    "    # Training typically does not only happen for one episode, but rather for many episodes. \n",
    "    # Create a function that first initializes your Q-values using your method initialize values,\n",
    "    # and then proceeds to train using a predetermined number of episodes using your method train episode \n",
    "    # In addition, at the end of each episode, you should test the greedy policy you've obtained \n",
    "    # using your method test_greedy. \n",
    "    # Keep track of this progress in a list or a numpy array, and\n",
    "    # return it in this procedure once training completes\n",
    "    \n",
    "    \"\"\"\n",
    "The algorithms in model-free RL are inherently random due to the randomness of\n",
    "the exploration policy and possibly the stochasticity of the MDP (not relevant here,\n",
    "since it is deterministic). In order to get a better idea of how fast each algorithm\n",
    "converges, practitioners typically run each algorithm independently using the same\n",
    "initial conditions many times (try to do it for at least 20 trials if you can: if your\n",
    "computer is too slow, you may reduce it to 10), and average the results from all the\n",
    "trials. In order to do this, you will have to run your method train trial many\n",
    "times. Record the per-episode performance data returned by that method over all\n",
    "trials. At the end, compute both the sample mean and standard deviation over trials\n",
    "for each episode. Plot these result in separate figures with episode on the x-axis and\n",
    "the mean/std of returns on the y-axis.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each algorithm you implement, you should have another Python file that allows to\n",
    "run your experiment. Here, you will have to initialize your domain, and the algorithm\n",
    "with your parameter settings, and run your algorithm.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA (On-Policy TD)\n",
    "Implement the on-policy one-step Sarsa algorithm as discussed in class. You may fix the learning rate to alpha = 0.3 for all your experiments. Set gamma = 0.99, the maximum episode length to 200 time steps, and the number of episodes of training to 500. Experiment with epsilon in {0.01, 0.1, 0.25}, answer the same questions and provide the same information as Part B and C.\n",
    "<img src=\"sarsa.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "class Sarsa:\n",
    "    def __init__(self, domain, alpha, gamma, epsilon):\n",
    "    # sets the domain instance, and all relevant parameters for each algorithm \n",
    "    # (e.g. learning rate, epsilon, etc.).\n",
    "\n",
    "    def initialize_values(self):\n",
    "    # A procedure called initialize values that initializes the Q-values to zero for all\n",
    "    # states in the MDP and all relevant other counters and memory.\n",
    "    \n",
    "    def sample_epsilon_greedy(self):\n",
    "    # takes a state of the MDP as argument and returns an action sampled \n",
    "    # according to the epsilon-greedy policy defined by the epsilon parameter \n",
    "    # set in the init method and the Q-values.\n",
    "    \n",
    "    def test_greedy(self): \n",
    "    # generates an episode from the MDP using the greedy policy \n",
    "    # (we are testing, so please do NOT use the epsilon-greedy policy for this!) \n",
    "    # with respect to the current Q-values, and returns G0, the discounted return \n",
    "    # starting from the initial state (please refer to your lecture or tutorial notes). \n",
    "    # This function will be used to inspect the rate at which your algorithm \n",
    "    # converges to a good policy as it trains.\n",
    "    \n",
    "    def train_episode(self):\n",
    "    # Each algorithm has a main training loop that generates an episode from the MDP\n",
    "    # according to the epsilon-greedy policy and uses this to update the Q-values. \n",
    "    # How you implement this method is up to you, but it is easiest to refer to the pseudo-code\n",
    "    # of each algorithm discussed in class or in the Sutton and Barto book.\n",
    "    \n",
    "    def train_trial(self):\n",
    "    # Training typically does not only happen for one episode, but rather for many episodes. \n",
    "    # Create a function that first initializes your Q-values using your method initialize values,\n",
    "    # and then proceeds to train using a predetermined number of episodes using your method train episode \n",
    "    # In addition, at the end of each episode, you should test the greedy policy you've obtained \n",
    "    # using your method test_greedy. \n",
    "    # Keep track of this progress in a list or a numpy array, and\n",
    "    # return it in this procedure once training completes\n",
    "    \n",
    "    \"\"\"\n",
    "The algorithms in model-free RL are inherently random due to the randomness of\n",
    "the exploration policy and possibly the stochasticity of the MDP (not relevant here,\n",
    "since it is deterministic). In order to get a better idea of how fast each algorithm\n",
    "converges, practitioners typically run each algorithm independently using the same\n",
    "initial conditions many times (try to do it for at least 20 trials if you can: if your\n",
    "computer is too slow, you may reduce it to 10), and average the results from all the\n",
    "trials. In order to do this, you will have to run your method train trial many\n",
    "times. Record the per-episode performance data returned by that method over all\n",
    "trials. At the end, compute both the sample mean and standard deviation over trials\n",
    "for each episode. Plot these result in separate figures with episode on the x-axis and\n",
    "the mean/std of returns on the y-axis.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each algorithm you implement, you should have another Python file that allows to\n",
    "run your experiment. Here, you will have to initialize your domain, and the algorithm\n",
    "with your parameter settings, and run your algorithm.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(lambda)\n",
    "Implement the on-policy TD(lambda) algorithm using traces as discussed in class. You may fix the learning rate to alpha = 0.3 for all your experiments and set the trace parameter to lambda = 0.9. Set gamma = 0.99, the maximum episode length to 200 time steps, and the number of episodes of training to 500. Experiment with epsilon in {0.01, 0.1, 0.25}, answer the same questions and provide the same information as Part B, C, D.\n",
    "<img src=\"on-td-lambda.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "class TdLambda:\n",
    "    def __init__(self, domain, alpha, gamma, epsilon):\n",
    "    # sets the domain instance, and all relevant parameters for each algorithm \n",
    "    # (e.g. learning rate, epsilon, etc.).\n",
    "\n",
    "    def initialize_values(self):\n",
    "    # A procedure called initialize values that initializes the Q-values to zero for all\n",
    "    # states in the MDP and all relevant other counters and memory.\n",
    "    \n",
    "    def sample_epsilon_greedy(self):\n",
    "    # takes a state of the MDP as argument and returns an action sampled \n",
    "    # according to the epsilon-greedy policy defined by the epsilon parameter \n",
    "    # set in the init method and the Q-values.\n",
    "    \n",
    "    def test_greedy(self): \n",
    "    # generates an episode from the MDP using the greedy policy \n",
    "    # (we are testing, so please do NOT use the epsilon-greedy policy for this!) \n",
    "    # with respect to the current Q-values, and returns G0, the discounted return \n",
    "    # starting from the initial state (please refer to your lecture or tutorial notes). \n",
    "    # This function will be used to inspect the rate at which your algorithm \n",
    "    # converges to a good policy as it trains.\n",
    "    \n",
    "    def train_episode(self):\n",
    "    # Each algorithm has a main training loop that generates an episode from the MDP\n",
    "    # according to the epsilon-greedy policy and uses this to update the Q-values. \n",
    "    # How you implement this method is up to you, but it is easiest to refer to the pseudo-code\n",
    "    # of each algorithm discussed in class or in the Sutton and Barto book.\n",
    "    \n",
    "    def train_trial(self):\n",
    "    # Training typically does not only happen for one episode, but rather for many episodes. \n",
    "    # Create a function that first initializes your Q-values using your method initialize values,\n",
    "    # and then proceeds to train using a predetermined number of episodes using your method train episode \n",
    "    # In addition, at the end of each episode, you should test the greedy policy you've obtained \n",
    "    # using your method test_greedy. \n",
    "    # Keep track of this progress in a list or a numpy array, and\n",
    "    # return it in this procedure once training completes\n",
    "    \n",
    "    \"\"\"\n",
    "The algorithms in model-free RL are inherently random due to the randomness of\n",
    "the exploration policy and possibly the stochasticity of the MDP (not relevant here,\n",
    "since it is deterministic). In order to get a better idea of how fast each algorithm\n",
    "converges, practitioners typically run each algorithm independently using the same\n",
    "initial conditions many times (try to do it for at least 20 trials if you can: if your\n",
    "computer is too slow, you may reduce it to 10), and average the results from all the\n",
    "trials. In order to do this, you will have to run your method train trial many\n",
    "times. Record the per-episode performance data returned by that method over all\n",
    "trials. At the end, compute both the sample mean and standard deviation over trials\n",
    "for each episode. Plot these result in separate figures with episode on the x-axis and\n",
    "the mean/std of returns on the y-axis.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each algorithm you implement, you should have another Python file that allows to\n",
    "run your experiment. Here, you will have to initialize your domain, and the algorithm\n",
    "with your parameter settings, and run your algorithm.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
