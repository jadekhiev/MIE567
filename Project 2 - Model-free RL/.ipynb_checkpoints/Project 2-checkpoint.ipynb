{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flags Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Flags domain, time is broken up into discrete decision periods, and the world is a square area that is broken up into a 5-by-5 grid of cells, as illustrated in Figure 1 below. \n",
    "<img src=\"flagworld.png\">\n",
    "At the beginning of each period, a robot occupies one of the available cells. It is then allowed to choose a direction of movement (up, down, left or right), and moves to the corresponding adjacent cell in that direction. However, if the chosen action would take the robot outside the boundaries of the grid, the robot does not move but remains in the current cell. So far, the problem is just like the Gridworld domain you have already encountered in your first assignment. In each run, the robot begins in state S. <br>\n",
    "As illustrated in Figure 1, instead of a single destination G, there are five different flags, numbered 1, 2, 3, 4 and G, that must be collected in the correct order (first 1, then 2, etc... to G). Once the robot has collected all the  flags in the correct order and lands on cell G, the task is complete. If the robot collects the wrong flag at the wrong time (e.g. tries to collect 2 first, then 1...), the robot does not have to start over. Instead, the robot is penalized for taking this action, and the wrong flag is not collected (you may assume for training purposes, that the flags that have already been collected so far are kept). The main objective is to collect all the flags in the correct order in the least possible amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flags :\n",
    "    def initial_state (self):\n",
    "    # return the initial state of this MDP\n",
    "    \n",
    "    def get_all_states (self):\n",
    "    # return a list containing all the possible states in this MDP\n",
    "\n",
    "    def is_terminal (self, state ):\n",
    "    # this function should return a Boolean indicating\n",
    "    # whether or not state is a terminal state ; in other\n",
    "    # words , does the game end at the specified state\n",
    "    # or does the robot keep playing ?\n",
    "\n",
    "    def transition (self, state , action ):\n",
    "    # this function should simulate the intended behavior of the Flags domain\n",
    "    # in particular , given the specified state and action, \n",
    "    # this function should return a tuple containing two things :\n",
    "    # 1. the next state to which we are transitioning to in the next period\n",
    "    # 2. the reward obtained according to your reward function when transitioning to the next state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-Visit On-Policy MC\n",
    "For this algorithm, like all the others that follow, you train using the epsilon-greedy policy. You are allowed to fix the discount factor gamma to 0.99. Set the maximum length of each episode to 200 if the terminal state is not reached by this point. Training should proceed for a total of 2000 episodes. You need to experiment with the epsilon parameter in epsilon greedy. Run your experiment for different choices of this parameter in {0.01, 0:1, 0.25} and include plots in your report for each of these values. What is the best value of epsilon? Why? For each epsilon, did your algorithm converge at all? What was the final policy and Q-values that you typically obtain (typically, as in the majority of the trials)? Does this correspond to an optimal policy?\n",
    "<img src=\"fv-on-MC.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning (Off-Policy TD)\n",
    "Implement the off-policy Q-learning algorithm as discussed in class. You may fix the learning rate to alpha = 0.3 for all your experiments. Set gamma = 0.99, the maximum episode length to 200 time steps, and the number of episodes of training to 500. Experiment with epsilon in {0.01, 0.1, 0.25}, answer the same questions and provide the same information as Part B.\n",
    "<img src=\"ql.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA (On-Policy TD)\n",
    "Implement the on-policy one-step Sarsa algorithm as discussed in class. You may fix the learning rate to alpha = 0.3 for all your experiments. Set gamma = 0.99, the maximum episode length to 200 time steps, and the number of episodes of training to 500. Experiment with epsilon in {0.01, 0.1, 0.25}, answer the same questions and provide the same information as Part B and C.\n",
    "<img src=\"sarsa.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(lambda)\n",
    "Implement the on-policy TD(lambda) algorithm using traces as discussed in class. You may fix the learning rate to alpha = 0.3 for all your experiments and set the trace parameter to lambda = 0.9. Set gamma = 0.99, the maximum episode length to 200 time steps, and the number of episodes of training to 500. Experiment with epsilon in {0.01, 0.1, 0.25}, answer the same questions and provide the same information as Part B, C, D.\n",
    "<img src=\"on-td-lambda.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
